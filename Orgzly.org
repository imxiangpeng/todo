* WAIT AI中屏研发评估
SCHEDULED: <2025-12-22 Mon>

#+begin_quote
建议：相关工作都交由椰壳来完成
#+end_quote

驱动由椰壳实现
蓝牙音响功能，作为device设备，工作量可能略多一些
摄像头前置2m手动遮挡？如何实现？还是保护盖

扬声器MIC阵列回声消除

SDK更新


双屏设置(内置屏和hdmi)


AI智能体对接

语音引擎

语音遥控器谁家的





摄像头相关

物理按键

* DONE 梯慧眼云平台APP接口与联通DMP平台接口差异
CLOSED: [2025-12-29 Mon 08:47] SCHEDULED: <2025-12-22 Mon>

我们云平台仿造阿里规范标准实现。

所以，与联通 DMP 的还是有挺大差异的。

* KILL 重庆广电 NGB-J接口可行性
CLOSED: [2025-12-29 Mon 09:04] SCHEDULED: <2025-12-22 Mon>

#+begin_quote
已经否定，跟新光确认歌华也没有使用 NGB-J 接口，塞科调用的是qin 接口。

跟重庆沟通他们会整理之前 mstar 项目使用的 porting 接口由我们来实现。

#+end_quote

找国科确认NGB-J接口完整性以及DVB相关功能支持情况

自己android版本评估NGB-J稳定性

* DONE 移动集采移动框架中含有终端管理应用模块 STBManager (与tr069不同，海思空实现)
CLOSED: [2025-12-29 Mon 09:05] SCHEDULED: <2025-12-22 Mon>

这个不是 TR069 而是框架里面终端管理应用的， amlogic 平台代码中有实现逻辑

* DONE 山东语音在播放界面识别阶段存在无法显示的问题
CLOSED: [2026-01-07 Wed 11:36] SCHEDULED: <2025-12-29 Mon>

1 播放情况下基本必现，例如本地播放等
2 茁壮平台无问题，但是本地播放一样存在问题
3 dumpsys SurfaceFlinger  以及HWC
4 screenrecord/screencap
5 新版app放到310板子上没问题，旧版app放到320上也可以显示(与语音)

6 logcat surfaceflinger  hwc


hwc HI_FB -   HI_GFBG

ro.config.gfx2d_compress


dumpsys 
Surfaceflinger window activity



在语音没有提示情况下，通过input keyevent KEYCODE_VOLUNE_DOWN调出音量条，就可以看到都立马显示出来了

参考： [[https://hisupport.hisilicon.com/hisupport/#!services/workflow/instance/myapplication/18701652?jalorNode=myApplication][HSCP2025122666957]] (alpha检测误判引起)

* DONE 媒体服务器 wvp+zlmediakit
CLOSED: [2026-01-08 Thu 09:41] SCHEDULED: <2025-12-29 Mon>

https://github.com/648540858/wvp-GB28181-pro


https://gitee.com/pan648540858/wvp-GB28181-pro


https://github.com/ZLMediaKit/ZLMediaKit


https://mp.weixin.qq.com/s/wJJtFu9Kp9N70I4LEXBOJQ


https://mp.weixin.qq.com/s/sdJk8Aj56iIwODXvmcqvyQ


ABLMediaServer 可以替代 ZLMediaKit；文档更详细，更容易入手；
ry-wvp 可以替代wvp


流媒体我只推荐mediamtx

https://github.com/bluenviron/mediamtx




https://mp.weixin.qq.com/s/vRMoV2ePbIfZvm5wOGRr2g





https://mp.weixin.qq.com/s/4bqtmF242efDcbAPchuZEQ


一个RK3588方案

https://wiki.t-firefly.com/zh_CN/iCore-3588JQ/linux_compile.html


firefly aibox 方案

https://www.t-firefly.com/wiki

* DONE tunnelto tailscale cloudflared
CLOSED: [2025-12-29 Mon 08:53] SCHEDULED: <2025-12-29 Mon>

https://github.com/agrinman/tunnelto

tunnelto 应该并不怎么样，

* DONE [#A] rk3588 ffmpeg 硬解 zlmediakit部署
CLOSED: [2026-01-13 Tue 13:33] SCHEDULED: <2026-01-05 Mon>

mvp or ruyi vue pro srs

* DONE 更新卡尔曼文章链接
CLOSED: [2026-01-08 Thu 09:40] DEADLINE: <2026-02-02 Mon> SCHEDULED: <2026-02-02 Mon>

How a Kalman filter works, in pictures


https://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/

* TODO rk3588多路抽帧解码
SCHEDULED: <2026-01-17 Sat>

4. 这种方案的极限在哪里？
​虽然这种方案很强，但你还需要关注一个参数：内存 (RAM)。
​每一个解码器实例，即使它在等待 I 帧，也会预先分配一定的输入/输出缓存（Buffer Pool）。
​如果你开了 128 路，每路预留 5 个 1080P 的 Buffer，即便不解码，光占用的显存可能就会耗尽 8GB/16GB 的内存。
​建议技巧： 在创建 MPP 实例时，通过配置 MPP_DEC_SET_EXT_BUF_GROUP 限制每一路解码器的 Buffer 数量。对于只需要识别单帧的应用，可以将 Buffer 限制在最小（如 2-3 个），这样可以极大增加路数上限。








在 GStreamer 方案中，实现“只解码关键帧（I帧）”主要有两种思路：一种是使用 identity 插件过滤数据包标志位，另一种是利用 mppvideodec 的底层属性（如果你的固件版本支持）。
最推荐的方法是在解码器之前拦截非关键帧。
1. 命令行方案：使用 identity 过滤
在 GStreamer 的 Buffer 中，每一个数据包都有一个 delta-unit 标志。如果该标志为 True，说明它是一个非关键帧（P/B帧）。通过 identity 插件的 drop-buffer-flags 属性，我们可以直接在进入解码器前把它们丢掉。
gst-launch-1.0 rtspsrc location=rtsp://... ! rtph264depay ! h264parse ! \
    identity drop-buffer-flags=0x00002000 ! \
    mppvideodec ! \
    videoconvert ! rkximagesink

 * 关键点： 0x00002000 是 GST_BUFFER_FLAG_DELTA_UNIT 的十六进制值。
 * 效果： h264parse 会为每个帧打上标签，identity 看到非关键帧就直接丢弃。这样 mppvideodec 拿到的全是 I 帧。
2. 进阶方案：使用 mppvideodec 的 skip-frames (依赖版本)
如果你使用的是较新版本的瑞芯微 GStreamer 插件（如 gstreamer-rockchip），有些版本已经将底层的 skip_frames 功能暴露出来了。你可以通过 gst-inspect-1.0 mppvideodec 检查是否有该属性。
# 如果支持 skip-frames 属性
gst-launch-1.0 ... ! mppvideodec skip-frames=2 ! ...

 * 参数含义：
   * 0: 解码所有帧 (All)
   * 1: 跳过非参考帧 (Non-ref)
   * 2: 只解码关键帧 (Keyframes Only)
3. C++ 代码方案：使用 Probe（探针）过滤
如果你是在 C++ 里写 GStreamer 代码，最稳妥、最灵活的方法是在 h264parse 的输出 Pad 上挂一个 Pad Probe。
static GstPadProbeReturn pipe_probe_cb(GstPad *pad, GstPadProbeInfo *info, gpointer user_data) {
    GstBuffer *buffer = GST_PAD_PROBE_INFO_BUFFER(info);
    
    // 检查是否是增量单元（即非关键帧）
    if (GST_BUFFER_FLAG_IS_SET(buffer, GST_BUFFER_FLAG_DELTA_UNIT)) {
        return GST_PAD_PROBE_DROP; // 丢弃该 Buffer，不传给下游解码器
    }
    
    return GST_PAD_PROBE_OK;
}

// 在代码中关联 Probe
GstElement *parser = gst_bin_get_by_name(GST_BIN(pipeline), "h264parse_element");
GstPad *srcpad = gst_element_get_static_pad(parser, "src");
gst_pad_add_probe(srcpad, GST_PAD_PROBE_TYPE_BUFFER, pipe_probe_cb, NULL, NULL);

4. 方案对比建议
| 方案 | 优点 | 缺点 | 适用场景 |
|---|---|---|---|
| identity 命令行 | 极简，无需写代码 | 只能处理简单的 Buffer 标记 | 快速验证、简单预览 |
| skip-frames 属性 | 效率最高，底层原生支持 | 依赖特定的固件和驱动版本 | 如果驱动支持，这是首选 |
| C++ Pad Probe | 最灵活，可根据业务精确控制 | 需要写少量 C++ 代码 | 正式项目、AI 识别工程 |
为什么这样做可以实现“路数复用”？
当你用上述方案把 P/B 帧过滤掉后，解码器的负载会瞬间降低 90% 以上（假设 GOP=30，你每秒只解 1 帧）。虽然你开了 128 路 mppvideodec 实例，但 VPU 硬件在 96% 的时间里都是闲置的。
需要注意的是： 这样做之后，输出的画面会像幻灯片一样跳转（每秒跳一下），但这正是 AI 识别（如车辆检测、人流统计）最理想的输入方式。
你现在的 GStreamer 环境是 Ubuntu 还是 Debian？我可以帮你确认一下 mppvideodec 的属性版本。




* TODO gstreamer分叉管道在正常解码情况下，能否实现，解码数据实时显示，然后每隔5帧送给npu推理一次呢？
SCHEDULED: <2026-01-17 Sat>

完全可以实现。这种需求在安防和边缘计算中非常典型：显示端需要流畅度（实时解码），识别端需要效率（抽帧推理）。
在 GStreamer 中，这种结构被称为**“分叉管道（Tee Pipeline）”**。你可以把解码后的 YUV 数据流一分为二：一波送去显示，另一波抽帧后喂给 NPU。
1. 技术方案：tee + videorate
 * tee： 负责将数据流复制成两个分支。
 * videorate： 负责在推理分支降低帧率（例如从 30fps 降到 6fps，即每 5 帧取 1 帧）。
 * queue： 必须在每个分支开始处加一个队列，防止推理端的卡顿影响到显示端的流畅。
2. C++ 核心代码实现
#include <gst/gst.h>
#include <iostream>

int main(int argc, char *argv[]) {
    gst_init(&argc, &argv);

    // 构建 Pipeline 字符串
    // 分支 1 (Display): 直接显示，保持 30fps
    // 分支 2 (Inference): 通过 videorate 降为 5/1 帧，由 appsink 接收
    std::string pipeline_str = 
        "rtspsrc location=rtsp://your_url latency=100 ! rtph264depay ! h264parse ! mppvideodec ! "
        "tee name=t "
        "t. ! queue ! rkximagesink sync=false "  // 显示分支
        "t. ! queue ! videorate ! video/x-raw,framerate=5/1 ! " // 推理分支：抽帧
        "rockchiprga ! video/x-raw,format=RGB,width=640,height=640 ! "
        "appsink name=my_sink emit-signals=true max-buffers=1 drop=true";

    GError *error = NULL;
    GstElement *pipeline = gst_parse_launch(pipeline_str.c_str(), &error);

    // ... 省略 appsink 的回调连接和错误检查 (见上一个回答) ...

    gst_element_set_state(pipeline, GST_STATE_PLAYING);
    
    std::cout << "实时显示已启动，每 5 帧提取 1 帧进行 NPU 推理..." << std::endl;

    GMainLoop *loop = g_main_loop_new(NULL, FALSE);
    g_main_loop_run(loop);

    return 0;
}

3. 为什么这个方案更优？
| 特性 | 实现方式 | 优势 |
|---|---|---|
| 实时性 | rkximagesink sync=false | 渲染端不等待，保证画面零延迟预览。 |
| 抽帧控制 | videorate | 自动根据时间戳平滑丢帧，即使码率波动，也能维持稳定的推理频率。 |
| 隔离性 | queue | 推理分支如果因为 NPU 负载过高卡住了，显示分支依然会流畅运行，不会导致监控画面卡死。 |
| 硬件加速 | rockchiprga | 推理分支的缩放和转 RGB 全部由 RGA 硬件完成，不占用解码器的预览性能。 |
4. 专家级调优提示：
 * Buffer 共享： 在 RK3588 上，mppvideodec 出来的 Buffer 默认是物理连续的（DMA-BUF）。tee 插件只是复制了指针（引用计数），并不会产生内存拷贝。所以即便开了分支，显存带宽压力也几乎没有增加。
 * 同步问题： sync=false 在显示端很重要，它可以防止因为某路流时间戳异常导致整个 Pipeline 阻塞。
这种方案下，128 路显然跑不动（因为 1080P 全量解码 128 路超出了 VPU 极限），但在 32 路 1080P 下，这是最完美的展示+分析架构。
您是打算把这个逻辑集成到现有的监控客户端界面里吗？我可以提供如何将 rkximagesink 嵌入到特定窗口 ID (X11 Window ID) 的方法。


* TODO EasyAIoT（云边端一体化智能算法应用平台）
SCHEDULED: <2026-01-18 Sun>

https://gitee.com/jiang4yu/easyaiot

* TODO 周三梯慧眼云平台大量假离线软件问题原因说明
SCHEDULED: <2026-01-23 Fri>

* TODO 此芯交流
SCHEDULED: <2026-01-27 Tue>

应用场景
算法
生态，技术支持
解决方案


多开

高通闭源


16 pcie 槽

系统丰富



https://docs.radxa.com/orion/o6



后摩

天数智芯


解决方案




均胜机器人



运营商












* TODO Redroid 了解
SCHEDULED: <2026-02-02 Mon>

Redroid 简介
Redroid 是一个基于 Android 的开源项目，是多个云手机方案中的一种，旨在为开发者和用户提供一个灵活的 Android 环境。它可以在不同的平台上运行，包括桌面操作系统，让用户能够体验 Android 系统的功能和应用。


mkdir ~/redroid && cd ~/redroid
repo init -u https://github.com/redroid-rockchip/platform_manifests.git -b redroid-12.0.0 --depth=1 --git-lfs
repo sync -c
sudo apt install git-lfs
repo forall -g lfs -c git lfs pull



https://docs.radxa.com/rock5/rock5b/app-development/redroid

* TODO 联通集采项目后续规划
SCHEDULED: <2026-02-02 Mon>

产测切换linux版本
user版本


* TODO rknn cpp multi thread infer 在3580达到140帧
SCHEDULED: <2026-02-01 Sun>

https://github.com/leafqycc/rknn-cpp-Multithreading




* TODO 抽烟数据集和训练
SCHEDULED: <2026-02-01 Sun>

由于原版 COCO 数据集不包含“抽烟”类别，你需要使用针对该任务优化的第三方数据集。目前最主流、效率最高的方式是使用 YOLO (You Only Look Once) 系列模型（如最新的 YOLOv8, YOLOv10 或 YOLO11）。
以下是为你整理的资源获取及训练全流程指南：
1. 抽烟数据集获取 (YOLO 格式)
你可以直接在以下平台下载已经标注好的 YOLO 格式数据集（包含 .txt 标签和 data.yaml 配置文件）：
 * Roboflow Universe (推荐):
   * Smoking Detection Dataset: 这里有大量社区贡献的抽烟检测数据集，规模从几百张到上万张不等。搜索 "Smoking" 即可找到。
   * 优势： 直接提供 YOLOv5/v8/v11 格式下载，省去格式转换麻烦。
 * Kaggle:
   * Smoking and Drinking Dataset for YOLO: 包含高质量的抽烟与饮酒行为标注。
 * Mendeley Data:
   * Smoker Detection Dataset: 包含约 1120 张图像（吸烟 vs 不吸烟），常用于学术研究。
2. 模型训练步骤 (以 YOLOv8/v11 为例)
目前最易用且性能最强的是 Ultralytics 框架。
第一步：安装环境
pip install ultralytics

第二步：准备配置文件 (data.yaml)
如果你从 Roboflow 下载，这个文件会自动生成。内容大致如下：
path: ../datasets/smoking_data  # 数据集根目录
train: images/train
val: images/val

names:
  0: smoking  # 类别名称

第三步：编写训练脚本
你可以直接在终端运行，或者写一个简单的 Python 脚本：
终端运行：
yolo task=detect mode=train model=yolov8n.pt data=data.yaml epochs=100 imgsz=640

Python 脚本：
from ultralytics import YOLO

# 1. 加载预训练模型（建议从官方权重开始，收敛更快）
model = YOLO('yolov8n.pt') 

# 2. 开始训练
results = model.train(
    data='data.yaml', 
    epochs=100, 
    imgsz=640, 
    batch=16, 
    device=0  # 如果有GPU则设为0
)

3. 训练建议（针对抽烟场景）
 * 处理小目标： 香烟属于微小物体，建议在训练时将输入分辨率设定为 imgsz=640 甚至更高（如 1280），并考虑使用 YOLOv8-m 或 YOLOv8-l 等稍大一点的模型以提升精度。
 * 正负样本平衡： 抽烟检测容易误判“手摸嘴巴”或“吃东西”。在数据集中加入一些“手部靠近面部但未吸烟”的负样本图像，能显著降低误报率。
 * 多任务辅助： 如果只检测“香烟”很难，可以尝试同时检测“人（person）”和“香烟（cigarette）”，通过判断两者距离来确认“抽烟行为”。
4. 获取现成的预训练模型
如果你不想从头训练，Hugging Face 上也有一些用户上传的权重：
 * 搜索 Enos-123/smoking-detection，可以直接加载 best.pt 进行推理。
您是想在本地电脑训练（需要显卡），还是希望在云端（如 Google Colab）进行免费训练？


* TODO Huggingface上预训练抽烟模型
SCHEDULED: <2026-02-01 Sun>

https://huggingface.co/Enos-123/smoking-detection?hl=zh-CN



https://mp.weixin.qq.com/s/TZxmp_WvJ86MwSI3YC8cBQ


* TODO 在colab上训练抽烟模型
SCHEDULED: <2026-02-01 Sun>

在 Google Colab 上训练 YOLOv8/v11 是一个非常明智的选择，因为你可以免费使用 NVIDIA T4 GPU。
以下是专为 Colab 环境优化的全流程步骤，你可以直接复制到 Notebook 的单元格中运行：
1. 环境准备 (GPU 加速)
在 Colab 顶部菜单栏选择：修改 -> 笔记本设置 -> 硬件加速器 -> T4 GPU。
# 检查 GPU 是否就绪
!nvidia-smi

# 安装 Ultralytics 库
!pip install ultralytics

2. 获取抽烟数据集
在 Colab 中最快捷的方法是直接从 Roboflow 导入。
操作步骤：
 * 访问 Roboflow Smoking Detection Dataset。
 * 点击 Download Dataset。
 * 选择 YOLOv8 格式。
 * 选择 show download code，复制那一串 Python 代码。
在 Colab 中粘贴运行（示例代码）：
!pip install roboflow
from roboflow import Roboflow
rf = Roboflow(api_key="你的API_KEY") # 替换为你自己的 Key
project = rf.workspace("v-smooking").project("smoking-p61gu") # 这只是个例子
dataset = project.version(1).download("yolov8")

3. 开始训练
我们直接调用 YOLOv8 的命令行工具。建议使用 Medium (m) 模型，因为它在小目标（香烟）检测上比 Nano (n) 稳定得多。
from ultralytics import YOLO

# 加载预训练的 YOLOv8m 权重
model = YOLO('yolov8m.pt')

# 开始训练
# data=dataset.location + "/data.yaml" 会自动指向你刚刚下载的数据路径
results = model.train(
    data=f"{dataset.location}/data.yaml", 
    epochs=100, 
    imgsz=640, 
    plots=True,
    device=0  # 使用 GPU
)

4. 训练过程可视化
Colab 会自动在 runs/detect/train/ 文件夹下生成训练日志。你可以运行以下代码查看准确率曲线：
from IPython.display import Image, display
display(Image(filename=f"runs/detect/train/results.png", width=600))

5. 推理测试与下载权重
训练完成后，模型效果最好的权重文件位于 runs/detect/train/weights/best.pt。
测试一张图片：
# 运行推理
!yolo task=detect mode=predict model=runs/detect/train/weights/best.pt conf=0.25 source={dataset.location}/test/images

下载模型到本地：
from google.colab import files
files.download('runs/detect/train/weights/best.pt')

💡 Colab 训练避坑指南：
 * 连接断开： Colab 免费版可能会因为闲置断开连接。建议训练时不要关闭浏览器页签。
 * 存储空间： 如果你的数据集非常大，建议挂载 Google Drive：
   from google.colab import drive
drive.mount('/content/drive')

   然后将训练结果保存到驱动器，防止 Colab 实例重置导致数据丢失。
你需要我帮你写一段自动挂载 Google Drive 并保存权重的代码吗？


* TODO 在yolo11源码中进行模型训练
SCHEDULED: <2026-02-01 Sun>

既然你已经下载了 YOLO11 的源码（通常是从 Ultralytics 的 GitHub 仓库克隆的），那么你的目标是进行源码开发级训练。这种方式方便你修改网络结构（.yaml 文件）或损失函数。
以下是在 Colab 中基于源码环境进行训练的步骤：
1. 准备源码环境
首先，你需要进入源码目录并安装其依赖，这样系统会引用你下载的这套代码，而不是全局的 pip 包。
# 1. 进入你下载或上传的 YOLO11 源码根目录
# 假设你克隆到了 /content/ultralytics
%cd /content/ultralytics

# 2. 以可编辑模式安装（这一步很关键！）
# -e 选项表示“开发模式”，对源码的修改会立即生效
!pip install -e .

2. 配置数据集
由于你不在全局环境，建议将 data.yaml 中的路径写成绝对路径，避免源码路径层级嵌套导致的找不到文件。
你可以手动编辑 data.yaml：
train: /content/datasets/smoking/train/images
val: /content/datasets/smoking/valid/images
test: /content/datasets/smoking/test/images

nc: 1
names: ['smoking']

3. 启动训练（通过源码脚本）
在源码模式下，你有两种方式启动训练：
方式 A：使用 main 入口脚本（推荐）
在源码根目录下，你可以创建一个简单的 train.py 脚本：
# train.py
from ultralytics import ASSETS, YOLO

# 加载 YOLO11 的预训练权重
# 此时它会查找源码中相关的配置文件
model = YOLO("yolo11m.pt") 

if __name__ == '__main__':
    model.train(
        data="/content/datasets/data.yaml", # 你的数据集路径
        epochs=100,
        imgsz=640,
        device=0,
        project="my_smoking_project", # 结果保存的文件夹名
        name="yolo11_smoking_run"
    )

然后在 Colab 单元格运行：
!python train.py

方式 B：通过命令行
直接在源码目录下运行 CLI：
!yolo task=detect mode=train model=yolo11m.pt data=/content/datasets/data.yaml epochs=100 imgsz=640

4. 源码训练的进阶操作
既然用了源码，你可能想做一些自定义：
 * 修改网络结构：
   你可以进入 ultralytics/cfg/models/11/yolo11.yaml 修改通道数或层数。训练时加载该配置文件即可：
   model = YOLO("ultralytics/cfg/models/11/yolo11.yaml").load("yolo11m.pt")
 * 修改超参数：
   修改 ultralytics/cfg/default.yaml 中的 lr0 (学习率)、mosaic (数据增强系数) 等。
5. 在 Colab 中防止数据丢失
源码训练的结果默认会保存在源码目录下的 runs/ 文件夹。如果 Colab 断开，这些数据会丢失。建议将结果软连接到 Google Drive：
from google.colab import drive
drive.mount('/content/drive')

# 创建一个软连接，将 runs 目录指向 Google Drive
!ln -s /content/drive/MyDrive/yolo_results /content/ultralytics/runs

你现在是已经把源码上传到 Colab 了，还是需要我提供克隆 GitHub 仓库并自动配置的命令？


